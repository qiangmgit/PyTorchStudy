{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwXVhw6qtYSh"
      },
      "source": [
        "Firstly download this dataset from http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip, and then unzip and upload to Google Drive's folder:\n",
        "./drive/MyDrive/text_classification_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YacVdUT3so6W",
        "outputId": "23016423-9573-48bd-bb88-19532315e5dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTzT_d8jwXl7"
      },
      "source": [
        "Then we are going to do some raw data processing work. You don't need to run the following cell again if you alreay have the dataset ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk70SJYVskvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d40a02f-db0a-4e83-a101-0f97aade1717"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "workspace_path = \"./drive/MyDrive/text_classification_test\"\n",
        "original_dataset_path = workspace_path + \"/training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "# this will cause error like \"utf-8' codec can't decode bytes ...\" \n",
        "# tweetsDF = pd.read_csv(original_dataset_path, header = None)\n",
        "tweetsDF = pd.read_csv(original_dataset_path, engine = \"python\", header = None)\n",
        "\n",
        " # check the first few records\n",
        "print(tweetsDF.head(5))\n",
        "\n",
        "# counting records\n",
        "first_column = tweetsDF[0]\n",
        "print(first_column.value_counts())\n",
        "\n",
        "# using the first column's value as label\n",
        "tweetsDF[\"sentiment_category\"] = first_column.astype('category')\n",
        "tweetsDF[\"sentiment\"] = tweetsDF[\"sentiment_category\"].cat.codes\n",
        "\n",
        "# save the processed data\n",
        "tweetsDF.to_csv(workspace_path + \"/train-processed.csv\", header = None, index = None)\n",
        "# smaller dataset\n",
        "tweetsDF.sample(10000).to_csv(workspace_path + \"/train-processed-sample.csv\", header = None, index = None)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0  ...                                                  5\n",
            "0  0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1  0  ...  is upset that he can't update his Facebook by ...\n",
            "2  0  ...  @Kenichan I dived many times for the ball. Man...\n",
            "3  0  ...    my whole body feels itchy and like its on fire \n",
            "4  0  ...  @nationwideclass no, it's not behaving at all....\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "4    800000\n",
            "0    800000\n",
            "Name: 0, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_BJi85-yp1T"
      },
      "source": [
        "Then we are going to use torchtext to process the csv file and get the dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FheqzSTzy0TR",
        "outputId": "d378ce20-d921-4a0c-f204-255fc060f3db"
      },
      "source": [
        "# note newer torchtext's API has changed and we are only using the legacy API for learning purpose.\n",
        "import torchtext.legacy\n",
        "from torchtext.legacy import data\n",
        "import torch\n",
        "\n",
        "LABEL = data.LabelField()\n",
        "# check https://spacy.io/usage/linguistic-features#how-tokenizer-works for what is spacy.\n",
        "# Using spacy can improve the quality of the generated vocabulary.\n",
        "TWEET = data.Field(tokenize = 'spacy', lower = True)\n",
        "\n",
        "# these are just mapping the columns in the csv file\n",
        "fields = [('score', None), ('id', None), ('date', None), ('query', None), \n",
        "          ('name', None), ('tweet', TWEET), ('category', None), ('label', LABEL)]\n",
        "\n",
        "twitterDataset = data.TabularDataset(\n",
        "    path = workspace_path + \"/train-processed-sample.csv\",\n",
        "    format = \"CSV\",\n",
        "    fields = fields,\n",
        "    skip_header = False)\n",
        "\n",
        "# splitting the dataset\n",
        "(train_data, val_data, test_data) = twitterDataset.split(split_ratio = [0.8, 0.1, 0.1])\n",
        "\n",
        "print((len(train_data), len(test_data), len(val_data)))\n",
        "print(vars(train_data.examples[7]))\n",
        "\n",
        "# building the vocabulary\n",
        "vocab_size = 10000\n",
        "LABEL.build_vocab(train_data, max_size = vocab_size)\n",
        "TWEET.build_vocab(train_data, max_size = vocab_size)\n",
        "\n",
        "# note torchtext will add <unk> and <pad> in additional to the required vocabulary\n",
        "# search for more info if needed\n",
        "print(\"length of vocabulary\")\n",
        "print(len(TWEET.vocab))\n",
        "\n",
        "# check the most common words\n",
        "print(\"top 10 common words\")\n",
        "print(TWEET.vocab.freqs.most_common(10))\n",
        "\n",
        "# Now create the dataloader\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "batch_size = 32\n",
        "\n",
        "train_data_iter, val_data_iter, test_data_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    device = device,\n",
        "    # note the following are necessary in training, otherwise\n",
        "    # training process will raise error saying\n",
        "    # sorting can't be done in the tweet\n",
        "    sort_key = lambda x: len(x.tweet),\n",
        "    sort_within_batch = False\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8000, 1000, 1000)\n",
            "{'tweet': ['i', 'do', 'nt', 'know', 'what', 'to', 'do', 'on', 'my', 'lame', 'fridaay', ' ', 'maybe', 'go', 'out', 'with', 'my', 'besties', '!', '!'], 'label': '0'}\n",
            "length of vocabulary\n",
            "10002\n",
            "top 10 common words\n",
            "[('i', 5017), ('!', 4455), ('.', 3972), (' ', 2966), ('to', 2802), ('the', 2575), (',', 2371), ('a', 1811), ('my', 1529), ('you', 1498)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R43_QUBIyvY"
      },
      "source": [
        "Now we are going to define the LSTM model (search for LSTM for its definition and advantages in natural language processing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFn24VZXJOvz",
        "outputId": "6b16d3b0-5b24-476e-b2f4-15070d8c3cd3"
      },
      "source": [
        "import torch.nn as nn\n",
        "class simpleLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
        "        super(simpleLSTM, self).__init__()\n",
        "        # search for the usage of Embedding if needed\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, num_layers = 1)\n",
        "        self.predictor = nn.Linear(hidden_size, 2)\n",
        "    \n",
        "    def forward(self, seq):\n",
        "        output, (hidden, _) = self.encoder(self.embedding(seq))\n",
        "        prediction = self.predictor(hidden.squeeze(0))\n",
        "        return prediction\n",
        "\n",
        "my_model = simpleLSTM(hidden_size = 100, embedding_dim = 300, vocab_size = 10002)\n",
        "my_model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "simpleLSTM(\n",
              "  (embedding): Embedding(10002, 300)\n",
              "  (encoder): LSTM(300, 100)\n",
              "  (predictor): Linear(in_features=100, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1OuiC8ZKbYM"
      },
      "source": [
        "Begin the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya_HmZgbKawA",
        "outputId": "09a4e191-f06b-4323-a462-723583ae3a6b"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(my_model.parameters(), lr = 0.02)\n",
        "\n",
        "def loss_update(model, batch, loss_fn):\n",
        "    output = model(batch.tweet)\n",
        "    loss = loss_fn(output, batch.label)\n",
        "    return loss\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_data_iter, val_data_iter, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    # this is to set model in training mode\n",
        "    model.train()\n",
        "    check_result = False\n",
        "    # training process\n",
        "    # note the difference here between the training for image processing\n",
        "    for _, batch in enumerate(train_data_iter):\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_update(model, batch, loss_fn)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_loss += loss.data.item() * batch.tweet.size(0)\n",
        "    training_loss /= len(train_data_iter)\n",
        "\n",
        "    # this is to set model in evaluation mode\n",
        "    model.eval()\n",
        "    for _, batch in enumerate(val_data_iter):\n",
        "        loss = loss_update(model, batch, loss_fn)\n",
        "        valid_loss += loss.data.item() * batch.tweet.size(0)\n",
        "    valid_loss /= len(val_data_iter)\n",
        "\n",
        "    print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))\n",
        "\n",
        "train(my_model, optimizer, torch.nn.CrossEntropyLoss(), train_data_iter, val_data_iter, 10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training Loss: 24.69, Validation Loss: 12.90\n",
            "Epoch: 1, Training Loss: 24.21, Validation Loss: 13.26\n",
            "Epoch: 2, Training Loss: 22.73, Validation Loss: 13.41\n",
            "Epoch: 3, Training Loss: 21.61, Validation Loss: 13.75\n",
            "Epoch: 4, Training Loss: 20.98, Validation Loss: 13.67\n",
            "Epoch: 5, Training Loss: 19.83, Validation Loss: 14.78\n",
            "Epoch: 6, Training Loss: 19.46, Validation Loss: 14.15\n",
            "Epoch: 7, Training Loss: 19.05, Validation Loss: 14.21\n",
            "Epoch: 8, Training Loss: 18.72, Validation Loss: 15.27\n",
            "Epoch: 9, Training Loss: 18.36, Validation Loss: 16.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jKeeMGySHke"
      },
      "source": [
        "The prediction in torchtext is not as trivial as in torchvision. Here is the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKS8KNhTS2Jv",
        "outputId": "a28bd363-f8bf-4c97-eaef-70fac81e29ef"
      },
      "source": [
        "my_model.to(\"cpu\")\n",
        "def classify_tweet(tweet, model):\n",
        "    categories = {0 : \"Negative\", 1 : \"Postive\"}\n",
        "    TWEET.preprocess(tweet)\n",
        "    prediction = model(TWEET.process([TWEET.preprocess(tweet)]))\n",
        "    print(\"the tweet is \")\n",
        "    print(tweet)\n",
        "    print(prediction)\n",
        "    print(\"the prediction is \")\n",
        "    print(categories[prediction.argmax().item()])\n",
        "    return \n",
        "\n",
        "\n",
        "\n",
        "tweet1 = \"Forgot to bring socks to the gym. I bet I get blisters!\"\n",
        "tweet2 = \"Wishing I could sneak in to watch the Star Trek premiere\"\n",
        "\n",
        "classify_tweet(tweet1, my_model)\n",
        "classify_tweet(tweet2, my_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the tweet is \n",
            "Forgot to bring socks to the gym. I bet I get blisters!\n",
            "tensor([[ 2.5766, -2.7281]], grad_fn=<AddmmBackward>)\n",
            "the prediction is \n",
            "Negative\n",
            "the tweet is \n",
            "Wishing I could sneak in to watch the Star Trek premiere\n",
            "tensor([[-1.2038,  1.4498]], grad_fn=<AddmmBackward>)\n",
            "the prediction is \n",
            "Postive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrxi6WBpwov"
      },
      "source": [
        "In the following we are going to demonstrate a few ways for data augmentation. We are not going to apply them in the training in this notebook, but you can always test it out when you have time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1DiHl3TpukK",
        "outputId": "f4742f91-76f1-4873-d903-c02214aa0944"
      },
      "source": [
        "import random\n",
        "from random import randrange\n",
        "\n",
        "my_sentence = \"The cat sat on the mat\"\n",
        "\n",
        "# split the sentence into words\n",
        "def get_words_lists(x):\n",
        "    return x.split()\n",
        "\n",
        "# get a random word from choices available\n",
        "def get_random_word():\n",
        "    random_word_list = [\"apple\", \"banana\", \"candy\", \"drink\", \"egg\", \"fish\", \"grape\"]\n",
        "    return random_word_list[randrange(0, len(random_word_list))]\n",
        "\n",
        "def random_insertion(sentence, n):\n",
        "    words = get_words_lists(sentence)\n",
        "    for _ in range(n):\n",
        "        words.insert(randrange(0, n), get_random_word())\n",
        "    # combine the words back into one sentence\n",
        "    return ' '.join(word for word in words)\n",
        "\n",
        "\n",
        "my_new_sentence = random_insertion(my_sentence, 3)\n",
        "print(my_new_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apple The egg banana cat sat on the mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDkYbkVks7GU",
        "outputId": "6b465779-9579-42c9-8846-6d2122d5c4c9"
      },
      "source": [
        "def random_deletion(sentence):\n",
        "    words = get_words_lists(sentence)\n",
        "    words.pop(randrange(0, len(words)))\n",
        "    # combine the words back into one sentence\n",
        "    return ' '.join(word for word in words)\n",
        "\n",
        "\n",
        "my_new_sentence = random_deletion(my_sentence)\n",
        "print(my_new_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cat sat on the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNNZKapItYnz",
        "outputId": "62cfb82f-c29f-497b-f225-2b07aa9f86c2"
      },
      "source": [
        "def random_swap(sentence):\n",
        "    words = get_words_lists(sentence)\n",
        "    index1 = randrange(0, len(words))\n",
        "    index2 = index1\n",
        "    while(index2 == index1):\n",
        "        index2 = randrange(0, len(words))\n",
        "    words[index1], words[index2] = words[index2], words[index1]\n",
        "\n",
        "    # combine the words back into one sentence\n",
        "    return ' '.join(word for word in words)\n",
        "\n",
        "\n",
        "my_new_sentence = random_swap(my_sentence)\n",
        "print(my_new_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sat cat on the mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5OzQ773t6mS"
      },
      "source": [
        "In the following we are going to try to use Google Translate Service for Data Augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2z8eBNiuLVq",
        "outputId": "1f23e011-3f31-46f3-8a62-8241c12c8aca"
      },
      "source": [
        "# install googletrans, newer version may have some API issue\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.8.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF0mllWst551"
      },
      "source": [
        "# NOTE calling this too often would cause Google Translate to stop serving you for a while...\n",
        "import googletrans\n",
        "import random\n",
        "from random import randrange\n",
        "\n",
        "def tranlate_call(sentence_list, dest_language):\n",
        "    translation_result = translator.translate(sentence_list, dest = dest_language)\n",
        "    translation_sentence = [word.text for word in translation_result]\n",
        "    print(translation_sentence)\n",
        "    return(translation_sentence)\n",
        "\n",
        "translator = googletrans.Translator()\n",
        "my_sentence = ['The cat sat on the mat']\n",
        "\n",
        "my_sentence_cn = tranlate_call(my_sentence, 'zh-CN')\n",
        "\n",
        "# translate back into English\n",
        "my_sentence_en = tranlate_call(my_sentence_cn, 'en')\n",
        "\n",
        "# translate into a random language\n",
        "available_langs = list(googletrans.LANGUAGES.keys())\n",
        "tr_lang = random.choice(available_langs)\n",
        "print(f\"Translating to {googletrans.LANGUAGES[tr_lang]}\")\n",
        "my_sentence_random = tranlate_call(my_sentence, tr_lang)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}